#################################################
# THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################

import warnings

import torch
from torch import nn
from torch.nn.modules.loss import _WeightedLoss  # noqa: PLC2701


def to_one_hot(
    labels: torch.Tensor, num_classes: int, dtype: torch.dtype = torch.float, dim: int = 1
) -> torch.Tensor:
    # if `dim` is bigger, add singleton dim at the end
    if labels.ndim < dim + 1:
        shape = list(labels.shape) + [1] * (dim + 1 - len(labels.shape))
        labels = torch.reshape(labels, shape)

    sh = list(labels.shape)

    if sh[dim] != 1:
        raise AssertionError("labels should have a channel with length equal to one.")

    sh[dim] = num_classes

    o = torch.zeros(size=sh, dtype=dtype, device=labels.device)

    return o.scatter_(dim=dim, index=labels.long(), value=1)  # labels


class PolyLoss(_WeightedLoss):
    def __init__(
        self,
        softmax: bool = True,
        weight: torch.Tensor | None = None,
        reduction: str = "mean",
        epsilon: float = 1.0,
    ) -> None:
        super().__init__()
        self.softmax = softmax
        self.reduction = reduction
        self.epsilon = epsilon
        self.cross_entropy = nn.CrossEntropyLoss(weight=weight, reduction="none")

    def forward(self, inputs: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """Poly Loss function.
        Args:
            inputs: the shape should be BNH[WD], where N is the number of classes.
                You can pass logits or probabilities as input, if pass logit, must set softmax=True
            target: if target is in one-hot format, its shape should be BNH[WD],
                if it is not one-hot encoded, it should has shape B1H[WD] or BH[WD],
                where N is the number of classes,
                It should contain binary values.

        Raises
        ------
            ValueError: When ``self.reduction`` is not one of ["mean", "sum", "none"].
        """
        if len(inputs.shape) - len(target.shape) == 1:
            target = target.unsqueeze(1).long()
        n_pred_ch, n_target_ch = inputs.shape[1], target.shape[1]
        # target not in one-hot encode format, has shape B1H[WD]
        if n_pred_ch != n_target_ch:
            print(torch.squeeze(target, dim=1).long())
            # squeeze out the channel dimension of size 1 to calculate ce loss
            self.ce_loss = self.cross_entropy(inputs, torch.squeeze(target, dim=1).long())
            # convert into one-hot format to calculate ce loss
            # target = to_one_hot(target, num_classes=n_pred_ch)
            print("all g")
        else:
            # # target is in the one-hot format, convert to BH[WD] format to calculate ce loss
            self.ce_loss = self.cross_entropy(inputs, torch.argmax(target, dim=1))
        if self.softmax:
            if n_pred_ch == 1:
                warnings.warn("single channel prediction, `softmax=True` ignored.")  # noqa: B028
            else:
                inputs = torch.softmax(inputs, 1)

        pt = (inputs * target).sum(dim=1)  # BH[WD]
        poly_loss = self.ce_loss + self.epsilon * (1 - pt)

        if self.reduction == "mean":
            polyl = torch.mean(poly_loss)  # the batch and channel average
        elif self.reduction == "sum":
            polyl = torch.sum(poly_loss)  # sum over the batch and channel dims
        elif self.reduction == "none":
            # BH[WD]
            polyl = poly_loss
        else:
            raise ValueError(
                f"Unsupported reduction: {self.reduction}, "
                "available options are ['mean', 'sum', 'none']."
            )

        return polyl


# class PolyLoss(nn.Module):
#     def __init__(
#         self, num_classes: int, epsilon: float = 1.0, reduction: str = "none", weight: T
# ensor = None
#     ):
#         """Create instance of Poly1CrossEntropyLoss.
#         :param num_classes:
#         :param epsilon:
#         :param reduction: one of none|sum|mean, apply reduction to final loss tensor
#         :param weight: manual rescaling weight for each class, passed to Cross-Entropy loss.
#         """
#         super().__init__()
#         self.num_classes = num_classes
#         self.epsilon = epsilon
#         self.reduction = reduction
#         self.weight = weight

#     def forward(self, logits, labels):
#         """Forward pass.
#         :param logits: tensor of shape [N, num_classes]
#         :param labels: tensor of shape [N]
#         :return: poly cross-entropy loss.
#         """
#         labels_onehot = F.one_hot(labels, num_classes=self.num_classes).to(
#             device=logits.device, dtype=logits.dtype
#         )
#         pt = torch.sum(labels_onehot * F.softmax(logits, dim=-1), dim=-1)
#         CE = F.cross_entropy(input=logits, target=labels, reduction="none", weight=self.weight)
#         poly1 = CE + self.epsilon * (1 - pt)
#         if self.reduction == "mean":
#             poly1 = poly1.mean()
#         elif self.reduction == "sum":
#             poly1 = poly1.sum()
#         return poly1
